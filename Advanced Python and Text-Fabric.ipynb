{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Python and Text-Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Christian HÃ¸jgaard Jensen (chj@dbi.edu)\n",
    "\n",
    "*Adapted from Martijn Naaijer (https://github.com/MartijnNaaijer/Shebanq_Course_Files/blob/master/Introduction_to_text_fabric.ipynb)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf.app import use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using etcbc/bhsa/tf - c r1.4 in C:\\Users\\Ejer/text-fabric-data\n",
      "Using etcbc/phono/tf - c r1.1 in C:\\Users\\Ejer/text-fabric-data\n",
      "Using etcbc/parallels/tf - c r1.1 in C:\\Users\\Ejer/text-fabric-data\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Documentation:** <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa\" title=\"provenance of BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis\">BHSA</a> <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Writing/Hebrew\" title=\"('Hebrew characters and transcriptions',)\">Character table</a> <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/0_home.html\" title=\"BHSA feature documentation\">Feature docs</a> <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/Bhsa/\" title=\"bhsa API documentation\">bhsa API</a> <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/\" title=\"text-fabric-api\">Text-Fabric API 7.0.3</a> <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#search-templates\" title=\"Search Templates Introduction and Reference\">Search Reference</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<details open><summary><b>Loaded features</b>:</summary>\n",
       "<p><b>BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis</b>: <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/book.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\book.tf\">book</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/book@am.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\book@am.tf\">book@ll</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/chapter.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\chapter.tf\">chapter</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/code.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\code.tf\">code</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/det.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\det.tf\">det</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/dist.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\dist.tf\">dist</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/dist_unit.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\dist_unit.tf\">dist_unit</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/domain.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\domain.tf\">domain</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/freq_lex.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\freq_lex.tf\">freq_lex</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/freq_occ.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\freq_occ.tf\">freq_occ</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/function.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\function.tf\">function</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/g_word.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\g_word.tf\">g_word</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/g_word_utf8.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\g_word_utf8.tf\">g_word_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/gloss.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\gloss.tf\">gloss</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/gn.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\gn.tf\">gn</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/instruction.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\instruction.tf\">instruction</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/is_root.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\is_root.tf\">is_root</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/kind.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\kind.tf\">kind</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/label.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\label.tf\">label</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/language.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\language.tf\">language</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/lex.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\lex.tf\">lex</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/lex_utf8.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\lex_utf8.tf\">lex_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/ls.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\ls.tf\">ls</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/nametype.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\nametype.tf\">nametype</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/nme.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\nme.tf\">nme</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/nu.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\nu.tf\">nu</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/number.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\number.tf\">number</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/otype.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\otype.tf\">otype</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/pargr.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\pargr.tf\">pargr</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/pdp.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\pdp.tf\">pdp</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/pfm.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\pfm.tf\">pfm</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/prs.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\prs.tf\">prs</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/prs_gn.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\prs_gn.tf\">prs_gn</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/prs_nu.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\prs_nu.tf\">prs_nu</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/prs_ps.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\prs_ps.tf\">prs_ps</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/ps.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\ps.tf\">ps</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/qere.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\qere.tf\">qere</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/qere_trailer.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\qere_trailer.tf\">qere_trailer</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/qere_trailer_utf8.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\qere_trailer_utf8.tf\">qere_trailer_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/qere_utf8.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\qere_utf8.tf\">qere_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/rank_lex.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\rank_lex.tf\">rank_lex</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/rank_occ.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\rank_occ.tf\">rank_occ</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/rela.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\rela.tf\">rela</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/root.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\root.tf\">root</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/sp.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\sp.tf\">sp</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/st.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\st.tf\">st</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/tab.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\tab.tf\">tab</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/trailer.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\trailer.tf\">trailer</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/trailer_utf8.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\trailer_utf8.tf\">trailer_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/txt.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\txt.tf\">txt</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/typ.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\typ.tf\">typ</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/uvf.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\uvf.tf\">uvf</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/vbe.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\vbe.tf\">vbe</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/vbs.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\vbs.tf\">vbs</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/verse.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\verse.tf\">verse</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/voc_lex.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\voc_lex.tf\">voc_lex</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/voc_lex_utf8.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\voc_lex_utf8.tf\">voc_lex_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/vs.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\vs.tf\">vs</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/vt.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\vt.tf\">vt</a>  <b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/distributional_parent.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\distributional_parent.tf\">distributional_parent</a></i></b>  <b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/functional_parent.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\functional_parent.tf\">functional_parent</a></i></b>  <b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/mother.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\mother.tf\">mother</a></i></b>  <b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/oslots.html\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/bhsa/tf/c\\oslots.tf\">oslots</a></i></b> </p><p><b>Parallel Passages</b>: <b><i><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/parallels/blob/master/programs/parallels.ipynb\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/parallels/tf/c\\crossref.tf\">crossref</a></i></b> </p><p><b>Phonetic Transcriptions</b>: <a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/phono/tf/c\\phono.tf\">phono</a>  <a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"C:\\Users\\Ejer/text-fabric-data/etcbc/phono/tf/c\\phono_trailer.tf\">phono_trailer</a> </p></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src: url('https://github.com/Dans-labs/text-fabric/blob/master/tf/server/static/fonts/SILEOT.ttf?raw=true');\n",
       "  src: url('https://github.com/Dans-labs/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff');\n",
       "}\n",
       "</style>\n",
       "\n",
       "<style type=\"text/css\">\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0a6611;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    direction: ltr;\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -0.1rem 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: x-small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 0.1em 0em;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-style: italic;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".verse {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: rtl;\n",
       "}\n",
       ".vl {\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-end;\n",
       "    align-items: flex-end;\n",
       "    direction: ltr;\n",
       "    width: 100%;\n",
       "}\n",
       ".outeritem {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: rtl;\n",
       "}\n",
       ".sentence,.clause,.phrase {\n",
       "    margin-top: -1.2em;\n",
       "    margin-left: 1em;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 0.3em;\n",
       "    border-style: solid;\n",
       "    border-radius: 0.2em;\n",
       "    font-size: small;\n",
       "    display: block;\n",
       "    width: fit-content;\n",
       "    max-width: fit-content;\n",
       "    direction: ltr;\n",
       "}\n",
       ".atoms {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    margin: 0.3em;\n",
       "    padding: 0.3em;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".satom,.catom,.patom {\n",
       "    margin: 0.3em;\n",
       "    padding: 0.3em;\n",
       "    border-radius: 0.3em;\n",
       "    border-style: solid;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".sentence {\n",
       "    border-color: #aa3333;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".clause {\n",
       "    border-color: #aaaa33;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".phrase {\n",
       "    border-color: #33aaaa;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".satom {\n",
       "    border-color: #aa3333;\n",
       "    border-width: 4px;\n",
       "}\n",
       ".catom {\n",
       "    border-color: #aaaa33;\n",
       "    border-width: 3px;\n",
       "}\n",
       ".patom {\n",
       "    border-color: #33aaaa;\n",
       "    border-width: 3px;\n",
       "}\n",
       ".word {\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    border-radius: 0.1em;\n",
       "    border: 1px solid #cccccc;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".lextp {\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    border-radius: 0.1em;\n",
       "    border: 2px solid #888888;\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".occs {\n",
       "    font-size: x-small;\n",
       "}\n",
       ".satom.l,.catom.l,.patom.l {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".satom.r,.catom.r,.patom.r {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".satom.L,.catom.L,.patom.L {\n",
       "    border-left-style: none\n",
       "}\n",
       ".satom.R,.catom.R,.patom.R {\n",
       "    border-right-style: none\n",
       "}\n",
       ".tr,.tr a:visited,.tr a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: large;\n",
       "    color: #000044;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".trb,.trb a:visited,.trb a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: normal;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".prb,.prb a:visited,.prb a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".h,.h a:visited,.h a:link {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    color: #000044;\n",
       "    direction: rtl;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".hb,.hb a:visited,.hb a:link {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    line-height: 1.8;\n",
       "    direction: rtl;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".vn {\n",
       "  font-size: small !important;\n",
       "  padding-right: 1em;\n",
       "}\n",
       ".rela,.function,.typ {\n",
       "    font-family: monospace;\n",
       "    font-size: small;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".pdp,.pdp a:visited,.pdp a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    color: #0000bb;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".voc_lex {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".vs {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".vt {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".gloss {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #444444;\n",
       "}\n",
       ".vrs {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: #444444;\n",
       "}\n",
       ".nd {\n",
       "    font-family: monospace;\n",
       "    font-size: x-small;\n",
       "    color: #999999;\n",
       "}\n",
       ".hl {\n",
       "    background-color: #ffee66;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<details open><summary><b>API members</b>:</summary>\n",
       "<a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#computed-data\" title=\"doc\">C Computed</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#computed-data\" title=\"doc\">Call AllComputeds</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#computed-data\" title=\"doc\">Cs ComputedString</a><br/>\n",
       "<a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#edge-features\" title=\"doc\">E Edge</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#edge-features\" title=\"doc\">Eall AllEdges</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#edge-features\" title=\"doc\">Es EdgeString</a><br/>\n",
       "<a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#loading\" title=\"doc\">TF</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#loading\" title=\"doc\">ensureLoaded</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#loading\" title=\"doc\">ignored</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#loading\" title=\"doc\">loadLog</a><br/>\n",
       "<a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#locality\" title=\"doc\">L Locality</a><br/>\n",
       "<a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#messaging\" title=\"doc\">cache</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#messaging\" title=\"doc\">error</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#messaging\" title=\"doc\">indent</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#messaging\" title=\"doc\">info</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#messaging\" title=\"doc\">reset</a><br/>\n",
       "<a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#navigating-nodes\" title=\"doc\">N Nodes</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#navigating-nodes\" title=\"doc\">sortKey</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#navigating-nodes\" title=\"doc\">otypeRank</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#navigating-nodes\" title=\"doc\">sortNodes</a><br/>\n",
       "<a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#node-features\" title=\"doc\">F Feature</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#node-features\" title=\"doc\">Fall AllFeatures</a>, <a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#node-features\" title=\"doc\">Fs FeatureString</a><br/>\n",
       "<a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#searching\" title=\"doc\">S Search</a><br/>\n",
       "<a target=\"_blank\" href=\"https://dans-labs.github.io/text-fabric/Api/General/#text\" title=\"doc\">T Text</a></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = use('bhsa', hoist=globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More features and tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know which range of slots is used for one specific object, you use sInterval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F.otype.sInterval('word'))\n",
    "print(F.otype.sInterval('phrase'))\n",
    "print(F.otype.sInterval('clause'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen the word feature lex, but there are more features, such as sp, part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_slot in range(1, 11):\n",
    "    print(F.sp.v(word_slot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that the range of phrases is (651503, 904689). What are the phrase types and phrase functions of the first ten phrases within this range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for phr in range(651542, 904748):\n",
    "    print(F.typ.v(phr), F.function.v(phr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In which book can you find word slot 100000? To be able to locate it, use T.sectionfromNode(), which returns a tuple with the book (by default in English), chapter and verse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.sectionFromNode(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need the name of the book in a different language, youse the argument \"lang\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.sectionFromNode(100000, lang = 'fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T.sectionFromNode returns a tuple. A tuple is also an ordered sequence of elements, but its values are immutable. It elements can be indexed with [] and a tuple can be recognized by the parentheses that embrace it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_tuple = (2, 4, 4, 5)\n",
    "print(this_tuple[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T.sectionFromNode() returns a tuple of length 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(T.sectionFromNode(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuple starts with the biblical book of a node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T.sectionFromNode(10)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen the feature \"lex\", which is a word feature. Other important word features are \"sp\" (part of speech), \"gn\" (gender), \"nu\" (number), \"vt\" (verbal tense), \"vs\" (verbal stem). The latter two have a value only if the word is a verb. Suppose you want to know the values of all of these features of the first 10 words of Genesis, we do this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in F.otype.s('word'):\n",
    "    if word < 11: # an alternative way of finding the first 10 words\n",
    "        print(F.lex.v(word), F.sp.v(word), F.gn.v(word), F.nu.v(word), F.vt.v(word), F.vs.v(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a feature has no value in the case of a specific word, NA is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets and counting the number of unique lexemes in the Hebrew Bible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set is another basic data type in Python. In contrast to the list it contains unique elements only without order. You can use a set if you want to know which unique elements there are in a large mount of data. First we look at a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_set = set() # an empty set is initialized\n",
    "print(integer_set) # prints the empty set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With .add() you can add an element to a set. In this case we add the integer 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_set.add(5)\n",
    "print(integer_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add another integer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_set.add(27)\n",
    "print(integer_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to add 5 again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_set.add(5)\n",
    "print(integer_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you see that the set still contains 27 and 5 only once, because the set becomes bigger only if a new unique element is added.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we investigate how many unique lexemes occur in the Hebrew parts of the Hebrew Bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_set = set()\n",
    "\n",
    "for word in F.otype.s('word'):\n",
    "    if F.language.v(word) == 'Hebrew':\n",
    "        lex_set.add(F.lex.v(word))\n",
    "    \n",
    "#print(lex_set) #uncomment this line if you want to see the whole set of lexemes. You will see that there is no specific order\n",
    "print(len(lex_set)) # returns the number of elements in the set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a set or a list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could have solved the problem of counting the unique lexemes in the Hebrew Bible with a list as well, by simply appending a lexeme to a list if it does not occur in the list already. The code would be only one line longer, but there is another difference, and that is the time it takes to do the calculation. Let us compare the time it would take using a list and a set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the list-implementation. First we import the class datetime from the module datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = datetime.now()  # the variable startTime saves the date and the time of the start of the execution of the cell\n",
    "\n",
    "lex_list = []\n",
    "\n",
    "for word in F.otype.s('word'):\n",
    "    if F.language.v(word) == 'Hebrew':\n",
    "        if not F.lex.v(word) in lex_list: # this is the extra line of code. It checks whether a lexeme occurs in the list already.\n",
    "            lex_list.append(F.lex.v(word)) \n",
    "\n",
    "print(len(lex_list))\n",
    "\n",
    "print(datetime.now() - startTime) # again, the time is measured and the starttime is subtracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we look at the set-implementation again and measure how long it takes to execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "lex_set = set()\n",
    "\n",
    "for word in F.otype.s('word'):\n",
    "    if F.language.v(word) == 'Hebrew':\n",
    "        lex_set.add(F.lex.v(word)) \n",
    "\n",
    "print(len(lex_set))\n",
    "\n",
    "print(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the set-implementation is much faster than the list-implementation. This is the case because checking whether a certain lexeme occurs already in the list takes more time when the list becomes longer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levels: L.u() and L.d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L in L.u() and L.d() stands for Layer, and u and d stand for up and down. Going up and down between linguistic layers is very important if you want to get access to various elements of linguistic units like clauses. In the figure below the use of L is schematically pictured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Layer_picture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a few examples to explore L.u():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_node = 1\n",
    "\n",
    "clause = L.u(word_node, 'clause')\n",
    "print(clause)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L.u() returns a tuple containing all relevant clauses, in this case one clause. We can extract the clause node using index [0]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clause[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L.d() works the opposite way, as it navigates down from one level to another level, e.g. from clause to phrase, or from book to chapter, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = L.d(clause[0], 'phrase')\n",
    "print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the tuple contains four elements, each element corresponding to one the four phrases contained in the clause. Using index [] we can access each of these phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(phrase[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use L.d() in combination with T.nodeFromSection() to find out which nodes belong to a certain section of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exod_1_1 = T.nodeFromSection(('Exodus', 1, 1))\n",
    "words_exod_1_1 = L.d(exod_1_1, 'word') # retrieve the word nodes with L.d()\n",
    "print(words_exod_1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print the text of a sequence of words with T.text(). T.text() iterates over a list of nodes, so you can either use range() to iterate over an interval or a list of word nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.text(range(0,4)) #Iterating over interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.text(words_exod_1_1) #Iterating over a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is nearly the same, but now we print the whole chapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exod_1 = T.nodeFromSection(('Exodus', 1, ))\n",
    "words_exod_1 = L.d(exod_1, 'word') # retrieve the word nodes with L.d()\n",
    "print(T.text(words_exod_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who is eating? Retrieving the subject of a predicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often you may not only interested be interested in the features of specific objects, but also of other objects in its environment. Suppose you are interested in eating habits in the Hebrew Bible. You decide to search for cases of the verb >KL[ (to eat), used as the predicate of a clause, and you are interested in those cases in which that clause has an explicit subject. You would like to know all the lexemes in the subject.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy is as follows. Fist we search for all cases of >KL[. From the word >KL[ we move upward to the phrase in which it occurs, using L.u(). Of this phrase it is checked if it is a predicate. We move upward again, to the level of the clause, and in the clause we check if there is an explicit subject, by moving downwards to the phrases in the clause, which is done with L.d()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in F.otype.s('word'):\n",
    "    if F.lex.v(word) == '>KL[':\n",
    "        phrase = L.u(word, 'phrase')[0] # L.u() returns a tuple.\n",
    "        # We want to know the slot of the phrase, so we add the index [0], which is the first value of the tuple.\n",
    "        \n",
    "        # now we check if the phrase is a predicate, we chech also for cases in a nominal predicate (PreC), \n",
    "        # and predicates with an object suffix (PreO):\n",
    "        if F.function.v(phrase) in {'Pred','PreC','PreO'}:\n",
    "            # we move upwards to the clause\n",
    "            clause = L.u(phrase, 'clause')[0]\n",
    "            # and we go down again to all the phrases in that clause\n",
    "            phrases = L.d(clause, 'phrase')\n",
    "            \n",
    "            # we loop over all the phrases to check if there is an explicit subject:\n",
    "            for phr in phrases:\n",
    "                if F.function.v(phr) == 'Subj':\n",
    "                    # we create an empty list, in which all lexemes are stored.\n",
    "                    lex_list = []\n",
    "                    \n",
    "                    # we move down to the lexemes of the subject:\n",
    "                    words = L.d(phr, 'word')\n",
    "                    \n",
    "                    for word in words:\n",
    "                        lex_list.append(F.lex.v(word))\n",
    "                        \n",
    "                    print(lex_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making structured datasets with text-fabric, a standard receipe\n",
    "\n",
    "In the following cells a csv file containing a number of features related to a topic of interest is created. The file is saved on the harddisk and can be processed further. The strategy of making this file is as follows.\n",
    "The features of an observation are stored in a list, and all these lists are stored in a dictionary:\n",
    "\n",
    "{id1 : [feat11, feat12, feat13, ...],  \n",
    "  id2 : [feat21, feat22, feat23, ...],  \n",
    "  id3 : [feat31, feat32, feat33, ...],  \n",
    "  ...  \n",
    "  ...  \n",
    " }\n",
    " \n",
    "The keys of this dictionary have to be unique of course, so in general it is convenient to use the tf-id of the object under investigation as key. The dictionary does not remember the order of the id's, so another structure is needed in which this order is remembered. We use a list to do this. The id's are added to the list in the canonical order in which our observations occur:\n",
    "\n",
    "[id1, id2, id3, ...]\n",
    "\n",
    "After all the observations are stored in the list and the dictionary the csv file is made by looping over the id's in the list, then for each id the feature list is fetched in the dictionary and added to the csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first show this with a very simple example, in which we make a csv file which contains all the places where the name JHWH occurs. The csv file will contain 4 columns: slot of the lexeme JHWH/, book, chapter, verse. On every row there will be one observation of the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['740', 'Genesis', '2', '4']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jhwh_list = []\n",
    "jhwh_dict = {}\n",
    "\n",
    "for word in F.otype.s('word'):\n",
    "    if F.lex.v(word) == 'JHWH/':\n",
    "        where = T.sectionFromNode(word)\n",
    "        info = [str(word), where[0], str(where[1]), str(where[2])]\n",
    "        jhwh_list.append(word) #The word-node\n",
    "        jhwh_dict[word] = info\n",
    "            \n",
    "print(len(jhwh_list)) # prints total number of occurrences of the name\n",
    "\n",
    "jhwh_dict[740]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'jhwh_data.csv', \"w\", encoding='utf-8') as csv_file:\n",
    "    \n",
    "    # it is often useful to make a header\n",
    "    header = ['slot', 'book', 'chapter', 'verse']\n",
    "    csv_file.write('{}\\n'.format(','.join(header)))\n",
    "\n",
    "    for case in jhwh_list:\n",
    "        info = jhwh_dict[case]\n",
    "        \n",
    "        # with .write() the information from info is added to the our file, but the elements in info need to be formatted first.\n",
    "        csv_file.write('{}\\n'.format(','.join(info))) #see below for explanation of this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join() and string formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last line of the previous code cell the information in the info-list was added to the csv file. How exactly was this done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First all the elements in the list were concatenated into one string with join(). join() concatenates everything in the list into one long string, and the separator is specified first. We want to make a csv file, so the comma is the natural choice, but you can use any character. Note that the list should contain only strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = ['Genesis', '1', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_string = (','.join(info))\n",
    "print(new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String formatting is used if you want to organize various kinds of information in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 5           # integer\n",
    "p = 'Amsterdam' # string\n",
    "f = True        # boolean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to put these together in one string, you use the placeholders {} and separate them with a character of choice. In the example below they are separated by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{} {} {}'.format(t, p, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'The temperature in {} is {} degrees'.format(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell in which the jhwh_data.csv file was made, you saw {}\\n . \\n is the newline. With this the next string that is added to the csv file comes on a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-structured datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous examples a so called structured dataset was made, which we saved in a csv file. The dataset is called structured, because it has a fixed format. It consists of a number of columns, and in each column you find the same kind of information for each case in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also be able to make unstructured or semi-structured datasets. An unstructured dataset contains data that are closer to the raw data as we find them in 'nature'. An example is a picture of a Dead Sea Scroll. In a semi-structured dataset the data are structured partly. In our case you could for instance make a text file with the consonantal text of the Hebrew Bible. In the following example, a text file is made in which the biblical text is represented per verse as a sequence of lexemes, separated by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lexemes.txt\", \"w\") as lex_file:    \n",
    "    for verse in F.otype.s('verse'):\n",
    "        where = T.sectionFromNode(verse)\n",
    "        # do not forget to make strings of chapter and verse\n",
    "        verse_string = '{} {} {} '.format(where[0], where[1], where[2])\n",
    "        words = L.d(verse, 'word')\n",
    "        \n",
    "        for word in words: # this is an alternative approach to string formatting\n",
    "            if word != words[-1]: # if the word is not the last word,\n",
    "                verse_string += F.lex.v(word) #add the lexeme\n",
    "                verse_string += ' ' # and add a space\n",
    "                \n",
    "            else: # this is the last word\n",
    "                verse_string += F.lex.v(word)\n",
    "                verse_string += '\\n' # add a newline\n",
    "                \n",
    "        lex_file.write(verse_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous script a file was made that contains the text of the whole MT. If you want to make a separate file for each book, you can do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexeme_processing(v):\n",
    "    \"\"\" \n",
    "    This function returns a string of lexemes for a verse node, which is the input.\n",
    "    It is identical to part of the code you have seen in the previous cell.\n",
    "    \"\"\"\n",
    "    where = T.sectionFromNode(v)\n",
    "    verse_string = '{} {} {} '.format(where[0], where[1], where[2])\n",
    "    \n",
    "    words = L.d(verse, 'word')\n",
    "    for word in words:\n",
    "        if word != words[-1]:\n",
    "            verse_string += F.lex.v(word)\n",
    "            verse_string += ' '\n",
    "        else:\n",
    "            verse_string += F.lex.v(word)\n",
    "            # a new verse gets a new line.\n",
    "            verse_string += '\\n'    \n",
    "    return(verse_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every book a new file is created.\n",
    "for book in F.otype.s('book'):\n",
    "    book_file = F.book.v(book) + '.txt'\n",
    "    \n",
    "    with open(book_file, \"w\") as new_file:\n",
    "        verses = L.d(book, 'verse')\n",
    "        for verse in verses:\n",
    "            # here the function lexeme_processing is called\n",
    "            new_string = lexeme_processing(verse)\n",
    "            new_file.write(new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final example of a structured dataset: the eat-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we continue with the >KL[ data. On every row in the csv file we store information about one clause containing >KL[, with the following information in columns:  \n",
    "slot of >KL[,  \n",
    "book,  \n",
    "chapter,  \n",
    "verse,  \n",
    "verbal tense,  \n",
    "verbal stem,  \n",
    "predicate type (Pred, PreC, PreO, PreS)  \n",
    "lexemes of the subject, concatenated in a string, separated by underscores.  \n",
    "The first 6 columns contain information about the verb, the predicate type contains information about the phrase in which >KL[ occurs, and the last column contains information about the subject of the clause. It may look like a lot of work, but you will notice that it is done straightforwardly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eat_list = []\n",
    "eat_dict = {}\n",
    "\n",
    "# this part is nearly identical to what you have already seen\n",
    "for w in F.otype.s('word'):\n",
    "    \n",
    "    # select the words with the right lexeme and make sure the language is Hebrew.\n",
    "    if F.lex.v(w) == '>KL[' and F.language.v(w) == 'Hebrew':\n",
    "        phrase = L.u(w, 'phrase')[0] \n",
    "        \n",
    "        # we include cases with a subjectsuffix\n",
    "        if F.function.v(phrase) == 'PreS':\n",
    "            suffix = F.prs.v(w)\n",
    "            # now we collect the information needed in a list called info\n",
    "            where = T.sectionFromNode(w)\n",
    "            info = [w, where[0], where[1], where[2], F.vt.v(w), F.vs.v(w), F.function.v(phrase), suffix]\n",
    "            eat_dict[w] = info\n",
    "            eat_list.append(w)\n",
    "            \n",
    "        # the other predicate types are processed now\n",
    "        else:\n",
    "            if F.function.v(phrase) in {'Pred','PreC','PreO'}:\n",
    "                where = T.sectionFromNode(w)\n",
    "                clause = L.u(phrase, 'clause')[0]\n",
    "                phrases = L.d(clause, 'phrase')\n",
    "                subject = False # we only include those cases that have an explicit subject\n",
    "                \n",
    "                for phr in phrases:\n",
    "                    if F.function.v(phr) == 'Subj':\n",
    "                        subject = True\n",
    "                        words = L.d(phr, 'word')\n",
    "                        words_lex = (F.lex.v(w) for w in words)\n",
    "                        subj_lexemes = \"_\".join(words_lex)\n",
    "                    \n",
    "                if subject: # this is the same as: if subject == True:, but it is a bit cleaner \n",
    "                    info = [w, where[0], where[1], where[2], F.vt.v(w), F.vs.v(w), F.function.v(phrase), subj_lexemes]\n",
    "                    eat_dict[w] = info\n",
    "                    eat_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'eat_data.csv', \"w\") as csv_file:\n",
    "    \n",
    "    # we make a header again\n",
    "    header = ['slot', 'book', 'chapter', 'verse', 'tense', 'stem', 'predicate', 'subj_lex']\n",
    "    csv_file.write('{}\\n'.format(','.join(header)))\n",
    "\n",
    "    for case in eat_list:\n",
    "        info_list = eat_dict[case]\n",
    "        line = [str(element) for element in info_list]\n",
    "        csv_file.write('{}\\n'.format(','.join(line)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Pandas module we can import csv files and explore them. First, we use the function `read_csv()` to import a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"eat_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `type()` to see the type of the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily display the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or perhaps we prefer to see only the first few rows. For this we use index []:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort our dataframe according to the values of the columns using the function `sort_values()`. The function takes an argument `by` for which the values can be either a string or a list depending on whether we want to sort our dataframe according to one or more columns. The order of the list stipulates the order of sorting, so the first column name in the list will be given highest priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.sort_values(by=['stem','subj_lex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display only one column using the index []:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['tense']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a a new dataframe from one of the columns in the dataframe. This dataframe will automatically count the different values of that column. The function `crosstab()` takes minimum two arguments, `index` and `columns`, where we assign the rows and the columns for our dataframe, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tense_tab = pd.crosstab(index=dataframe[\"tense\"], columns=\"count\")\n",
    "tense_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the total number of values in our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tense_tab.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use the sum of all values to create a proportional table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tense_tab/tense_tab.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-way tables\n",
    "\n",
    "We can also create a two-way table based on two of our columns in the dataframe. This is called cross tabulation. Here we want to cross tabulate our coloumns 'tense' and 'stem':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tense_stem = pd.crosstab(index=dataframe['tense'],columns=dataframe['stem'])\n",
    "\n",
    "tense_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the names of the columns and rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tense_stem.columns = ['hiphil','niphal', 'pual', 'qal']\n",
    "\n",
    "tense_stem.index = ['yiqtol', 'infc','qatal','ptca','wayq']\n",
    "\n",
    "tense_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily make marginal counts (that is, row counts and column counts) by adding the argument `margins = True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tense_stem = pd.crosstab(index=dataframe['tense'],columns=dataframe['stem'], margins=True)\n",
    "\n",
    "tense_stem.columns = ['hif','nif','pual','qal','rowtotal']\n",
    "tense_stem.index = ['impf','infc','perf','ptca','wayq','coltotal']\n",
    "\n",
    "tense_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can also display proportions rather than counts by dividing the table by the grand total. For this we need to identify the location of the marginal counts, and we do this using either `.loc` or `.iloc` for either labels (e.g. 'rowtotal') or integers (e.g. 4):\n",
    "\n",
    "`.loc` uses labels for index\n",
    "\n",
    "`.iloc` uses integers for index\n",
    "\n",
    "See more in [docs](http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated)\n",
    "\n",
    "The grandtotal will be the juncture of rowtotal and coltotal, so stipulate this specific location in our index []:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tense_stem/tense_stem.loc[\"coltotal\",\"rowtotal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tense_stem/tense_stem.iloc[5,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want the proportions of each column, divide by coltotal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tense_stem/tense_stem.loc['coltotal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the operator divides numbers in a dataframe on a row-to-row basis, which means that we have to change this setting, as we find the proportions on a column-by-column basis. We do so by using the function `.div()` and setting the argument `axis = 'index'` or `axis = 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tab = tense_stem.div(tense_stem['rowtotal'], axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display in percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tab*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function `round()` to round off the numbers. The first argument is the object to be rounded off, while the second argument is the number of decimals to be rounded off to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(new_tab*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, we do not want to go too deep into data modelling, statistics, and plotting of data, however interesting, since these subjects require a course on its own. For now, we will only see a few functions that can help visualizing counts and proportions in order to make preliminary observations.\n",
    "\n",
    "First, let us find the 20 most frequent subjects in our 'eat' data set.\n",
    "\n",
    "The first step is to create a table with subjects and counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_subj = pd.crosstab(index=dataframe['subj_lex'], columns='count')\n",
    "table_subj[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to sort the table in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_subj_sort = table_subj.sort_values(by=['count'],ascending=False)\n",
    "table_subj_sort[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want the 20 most frequent lexemes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20 = table_subj_sort[:20]\n",
    "top_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go plotting!\n",
    "\n",
    "We use another python package. See [docs](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard plot is very easy to create. We simply use the function `plot()` and require the plot to be a barplot, setting the kind='bar'.\n",
    "Finally, we display the plot with the function `show()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = top_20.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify our plot in many various ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = top_20.plot(kind='bar')\n",
    "\n",
    "plt.title('Top 20 subject lexemes of the verb \"eat\"') #Adding a title\n",
    "\n",
    "plt.ylabel('frequency') #adding Y-axis label\n",
    "plt.xlabel('subject lexeme') #changing X-axis label\n",
    "\n",
    "plt.legend(loc='best', shadow=True) #Modifying legend\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,5) #Modifying size of figure. X-value is width, Y-value is height.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from these few modifications, there is another thousand possibilities. Keep in mind, that the intention of a plot is to visualize specific data in order for a reader to quicly get a grasp of the results. For this, we will certainly need a title, X- and Y-axis labels, scales and possibly a legend. These are basic requirement and for most purposes these will also suffice. Remember this before you spend hours on the layout of the graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
